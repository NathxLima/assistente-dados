# -*- coding: utf-8 -*-
import os
import json
import time
from pathlib import Path

import bcrypt
import streamlit as st
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import HumanMessage, AIMessage


# ================== CONFIG INICIAL ==================
load_dotenv()
os.environ["STREAMLIT_WATCHER_TYPE"] = "none"
st.set_page_config(page_title="üé≤ Nathal.IA", layout="wide")
# ====================================================

# ================== AUTH (login/senha) ==================
@st.cache_data(show_spinner=False, ttl=30)
def carregar_usuarios_hash(auth_users_file: str) -> dict:
    if not auth_users_file:
        raise ValueError("AUTH_USERS_FILE vazio")

    arquivo = Path(auth_users_file)
    if not arquivo.exists():
        raise FileNotFoundError(f"Arquivo de usu√°rios n√£o encontrado: {arquivo}")

    return json.loads(arquivo.read_text(encoding="utf-8"))


def validar_login(usuario: str, senha: str, usuarios_hash: dict) -> bool:
    usuario = (usuario or "").strip()
    if not usuario:
        return False

    hash_str = usuarios_hash.get(usuario)
    if not hash_str:
        return False

    try:
        return bcrypt.checkpw(senha.encode("utf-8"), hash_str.encode("utf-8"))
    except Exception:
        return False


def gate_autenticacao():
    # estado inicial
    st.session_state.setdefault("autenticado", False)
    st.session_state.setdefault("usuario", "")
    st.session_state.setdefault("tentativas", 0)
    st.session_state.setdefault("bloqueado_ate", 0.0)

    # j√° autenticado ‚Üí segue o app
    if st.session_state["autenticado"]:
        return

    agora = time.time()
    if agora < st.session_state["bloqueado_ate"]:
        st.title("üîê Acesso Restrito ‚Äî Nathal.IA")
        st.warning("Muitas tentativas. Aguarde alguns segundos.")
        st.stop()

    # carrega usu√°rios
    auth_users_file = os.getenv("AUTH_USERS_FILE", "").strip()
    try:
        usuarios_hash = carregar_usuarios_hash(auth_users_file)
    except Exception as e:
        st.error(f"Erro de autentica√ß√£o: {e}")
        st.stop()

    # tela de login
    st.title("üîê Acesso Restrito ‚Äî Nathal.IA")
    with st.form("login_form"):
        usuario = st.text_input("Usu√°rio")
        senha = st.text_input("Senha", type="password")
        entrar = st.form_submit_button("Entrar")

    if entrar:
        if validar_login(usuario, senha, usuarios_hash):
            st.session_state["autenticado"] = True
            st.session_state["usuario"] = usuario.strip()
            st.session_state["tentativas"] = 0
            st.session_state["bloqueado_ate"] = 0.0
            st.rerun()
        else:
            st.session_state["tentativas"] += 1
            st.error("Usu√°rio ou senha inv√°lidos.")

            if st.session_state["tentativas"] >= 5:
                st.session_state["bloqueado_ate"] = time.time() + 20
                st.session_state["tentativas"] = 0

    st.stop()


# ‚úÖ CHAMAR A AUTENTICA√á√ÉO AQUI (logo ap√≥s definir)
gate_autenticacao()


def botao_logout():
    if st.session_state.get("autenticado"):
        if st.button("Sair"):
            st.session_state["autenticado"] = False
            st.session_state["usuario"] = ""
            st.rerun()


# ================== SELE√á√ÉO DE TEMA ==================
def identificar_tema(pergunta):
    TEMAS_DISPONIVEIS = [
        "machine_learning",
        "estatistica_basica",
        "inteligencia_artificial",
        "SQL",
        "programacao_python",
        "financas_credito",
        "negocios_geral",
        "mysql_escola",
        "global",
    ]

    pergunta = (pergunta or "").strip()
    if not pergunta:
        return "global"

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    melhor_tema = "global"
    melhor_score = float("inf")

    for tema in TEMAS_DISPONIVEIS:
        pasta = os.path.join("data", tema)
        if not os.path.exists(pasta):
            continue

        try:
            db = Chroma(persist_directory=pasta, embedding_function=embeddings)
            resultados = db.similarity_search_with_score(pergunta, k=1)

            if not resultados:
                continue

            _, score = resultados[0]
            if score < melhor_score:
                melhor_score = score
                melhor_tema = tema

        except Exception:
            continue

    return melhor_tema


# ================== UI ==================
st.markdown(
    """
<style>
html, body, [class*="css"] { font-family: 'Segoe UI', sans-serif; }
.chat-container { display: flex; flex-direction: column; margin-bottom: 80px; }
.bubble.user-msg {
    background-color: #343541; color: #fff; padding: 12px 16px;
    border-radius: 12px; margin: 8px 0; align-self: flex-end; max-width: 85%;
}
.bubble.bot-msg {
    background-color: #444654; color: #eee; padding: 12px 16px;
    border-radius: 12px; margin: 8px 0; align-self: flex-start; max-width: 85%;
}
</style>
""",
    unsafe_allow_html=True,
)

st.title("üé≤ Nathal.IA")
st.subheader("Da engenharia √† ci√™ncia de dados: sua parceira estrat√©gica em IA")

# (Opcional) bot√£o logout no topo
with st.sidebar:
    st.write(f"üë§ Usu√°rio: **{st.session_state.get('usuario', '')}**")
    botao_logout()

# mem√≥ria do chat
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        input_key="question",
        output_key="answer",
        return_messages=True,
    )

def mostrar_historico():
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    for msg in st.session_state.memory.chat_memory.messages:
        if isinstance(msg, HumanMessage):
            st.markdown(
                f'<div class="bubble user-msg">üß† Voc√™: {msg.content}</div>',
                unsafe_allow_html=True,
            )
        elif isinstance(msg, AIMessage):
            st.markdown(
                f'<div class="bubble bot-msg">ü§ñ Resposta: {msg.content}</div>',
                unsafe_allow_html=True,
            )
    st.markdown("</div>", unsafe_allow_html=True)

mostrar_historico()

with st.form("pergunta_form", clear_on_submit=True):
    nova_pergunta = st.text_input("Digite sua pergunta...", placeholder="O que voc√™ quer saber?")
    enviar = st.form_submit_button("Enviar")

if enviar and nova_pergunta:
    with st.spinner("ü§ñ Nathal.IA est√° pensando..."):
        try:
            # Identificar o tema
            tema = identificar_tema(nova_pergunta)

            # Caminho da base vetorial do tema
            pasta_vetorial = os.path.join("data", tema)
            if not os.path.exists(pasta_vetorial):
                # fallback seguro
                pasta_vetorial = os.path.join("data", "global")

            embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
            vetores = Chroma(persist_directory=pasta_vetorial, embedding_function=embeddings)

            # Retriever (k=4)
            retriever = vetores.as_retriever(search_kwargs={"k": 4})

            # LLM (GPT-4.1-mini)
            llm = ChatOpenAI(
                model="gpt-4.1-mini",
                temperature=0.15,
                max_tokens=900,
                api_key=os.getenv("OPENAI_API_KEY"),
            )

            # Prompt
            prompt = PromptTemplate(
                input_variables=["chat_history", "context", "question"],
                template="""
Voc√™ √© a Nathal.IA ‚Äî uma assistente estrat√©gica de dados criada por Nath√°lia Lima.

Seu papel √© apoiar decis√µes reais de neg√≥cio usando dados, estat√≠stica e machine learning.
Voc√™ responde como uma cientista de dados experiente, segura e pr√°tica, com vis√£o de neg√≥cio.

Princ√≠pios obrigat√≥rios:
- Responda sempre em portugu√™s.
- Priorize clareza, direcionamento e impacto no neg√≥cio.
- Demonstre dom√≠nio t√©cnico, explicando conceitos quando isso ajudar a tomar uma decis√£o melhor.
- Evite tom acad√™mico ou excessivamente professoral.
- N√£o ensine ‚Äúpor ensinar‚Äù: toda explica√ß√£o deve justificar uma escolha, um risco ou uma prioriza√ß√£o.
- S√≥ apresente m√∫ltiplos caminhos quando houver uma decis√£o real a ser feita.
- Nunca crie caminhos artificiais apenas para preencher resposta.

Estrutura esperada da resposta:
1) Contextualize rapidamente o problema de neg√≥cio.
2) Explique os conceitos t√©cnicos necess√°rios para embasar a decis√£o (sem excesso).
3) Organize as op√ß√µes relevantes, destacando trade-offs reais.
4) Finalize com uma recomenda√ß√£o clara, pr√°tica e acion√°vel.

Quando fizer sentido:
- Mostre trade-offs (vantagens, riscos, custos de erro).
- Relacione com m√©tricas, or√ßamento, capacidade operacional ou impacto financeiro.
- Utilize exemplos aplic√°veis a contextos reais (cr√©dito, cobran√ßa, churn, opera√ß√µes, dados).

Uso de documentos (RAG):
- Utilize os documentos fornecidos como base factual.
- Se n√£o houver evid√™ncia nos documentos, deixe isso expl√≠cito.
- N√£o extrapole al√©m do que os documentos sustentam.

Fontes espec√≠ficas:
- Se o usu√°rio mencionar explicitamente um autor, livro ou obra:
  - Utilize EXCLUSIVAMENTE os documentos dessa fonte.
  - Se nenhum trecho dessa fonte estiver presente no contexto recuperado,
    informe claramente que n√£o h√° evid√™ncia suficiente para responder.
  - Nunca utilize outras fontes como substitui√ß√£o.

Regra cr√≠tica de uso de exemplos:
- Exemplos, n√∫meros ou modelos mencionados nos documentos s√£o ilustrativos,
  a menos que o usu√°rio forne√ßa explicitamente dados do seu pr√≥prio problema.
- Nunca trate exemplos did√°ticos dos livros como resultados reais aplic√°veis.
- Nunca nomeie modelos como ‚ÄúA‚Äù ou ‚ÄúB‚Äù se eles n√£o existirem explicitamente no problema do usu√°rio.
- Se o documento trouxer apenas exemplos conceituais, deixe isso claro na resposta.

Postura profissional obrigat√≥ria:
- Responda como algu√©m que ser√° cobrado pelo resultado da decis√£o.
- Evite respostas neutras ou excessivamente abrangentes.
- Sempre deixe claro:
  ‚Ä¢ O que eu faria
  ‚Ä¢ O que eu N√ÉO faria
  ‚Ä¢ Por qu√™
- Se houver incerteza, explicite o risco e proponha mitiga√ß√£o.
- N√£o liste possibilidades sem hierarquiz√°-las.

Regra de senioridade:
- Engenharia de Dados ‚Üí foque em arquitetura, ordem de execu√ß√£o e falhas comuns.
- An√°lise de Dados ‚Üí foque em interpreta√ß√£o, prioriza√ß√£o e comunica√ß√£o.
- Ci√™ncia de Dados ‚Üí foque em custo de erro, m√©tricas certas e impacto operacional.
- Nunca misture pap√©is sem justificativa expl√≠cita.

Gera√ß√£o de c√≥digo:
- Gere c√≥digo somente quando isso ajudar a implementar, validar ou operacionalizar a decis√£o.
- Antes de apresentar c√≥digo, explique brevemente POR QUE essa abordagem t√©cnica √© adequada ao contexto.
- O c√≥digo deve ser funcional, organizado e alinhado ao ambiente mencionado pelo usu√°rio.
- Nunca gere c√≥digo gen√©rico sem conex√£o clara com o problema de neg√≥cio descrito.

Regra de sa√≠da (c√≥digo):
- Se o usu√°rio pedir explicitamente por c√≥digo (ex: ‚Äúmonte um c√≥digo‚Äù, ‚Äúme d√™ um script‚Äù, ‚Äúquero um exemplo‚Äù), forne√ßa c√≥digo completo e execut√°vel.
- Se o usu√°rio n√£o pedir c√≥digo, n√£o responda com c√≥digo por padr√£o; ofere√ßa no m√°ximo um pseudo-exemplo opcional ao final.

Regra de confiabilidade:
- Nunca presuma contexto operacional, m√©tricas, volumes ou resultados.
- Se algo n√£o estiver explicitamente descrito nos documentos ou na pergunta,
  trate como desconhecido.
- Prefira assumir incerteza a fornecer uma resposta imprecisa.

Encerramento:
- Sempre conclua com uma recomenda√ß√£o orientada √† decis√£o de neg√≥cio.
- Evite perguntas gen√©ricas.
- S√≥ fa√ßa perguntas ao usu√°rio se isso destravar uma escolha pr√°tica
  (ex: or√ßamento, volume de clientes, restri√ß√£o operacional).

Hist√≥rico da conversa:
{chat_history}

Contexto (documentos relevantes):
{context}

Pergunta:
{question}

Resposta:
""",
            )

            # Cadeia (SEM fontes)
            chain = ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=retriever,
                memory=st.session_state.memory,
                combine_docs_chain_kwargs={"prompt": prompt},
                return_source_documents=False,
                output_key="answer",
            )

            # Rodar consulta
            resultado = chain.invoke({"question": nova_pergunta})

            # Guardar resposta (opcional)
            st.session_state["last_answer"] = resultado.get("answer", "")

            # Rerun para renderizar no hist√≥rico
            st.rerun()

        except Exception as e:
            st.error(f"Erro ao gerar resposta: {e}")
